# è¿™ä¸ªæ˜¯DSæ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥è¿›è¡Œå¤„ç†çš„å•æ¡å¤„ç†ä»£ç ã€‚
import time
import torch
import pandas as pd
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer
import warnings
from torch.nn.functional import pad  # å·¦paddingåº“

warnings.filterwarnings("ignore") # å¿½ç•¥è­¦å‘Š,æ— è§†é£é™©,ç»§ç»­è¿è¡Œ

print("å¼€å§‹è¿è¡Œç¨‹åº...")

# æ¨¡å‹è·¯å¾„å’Œè®¾å¤‡
model_name = "../autodl-tmp/DeepSeek-R1-Distill-Qwen-7B"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def get_attn_implementation():
    capability = torch.cuda.get_device_capability()
    return "flash_attention_2" if capability[0] >= 8 else "eager"

attn_impl = get_attn_implementation()

# æ¨¡å‹åŠ è½½ å·¦æˆªæ–­
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True,
    padding_side="left"  # å¿…é¡»åœ¨è¿™é‡Œè®¾ç½®
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    torch_dtype=torch.float16 if device.type == "cuda" else torch.float32,
    device_map="auto" if device.type == "cuda" else None,
    attn_implementation=attn_impl if device.type == "cuda" else None
).to(device).eval()

# æ„é€ æç¤ºè¯
def build_prompt_tokens(raw_text, tokenizer, max_input_tokens):
    """
    æ„å»ºå®Œæ•´ prompt å¹¶è‡ªåŠ¨æŒ‰ token æˆªæ–­ raw_textï¼Œä»¥ç¡®ä¿æ€»é•¿ä¸è¶…è¿‡ max_input_tokens
    è¿”å›: input_idsï¼ˆList[int]ï¼‰
    """

    # System prompt
    system_content = (
        "ä½ æ˜¯åŒ»ç–—æ–‡æ¡£ç»“æ„æŠ½å–å·¥å…·ã€‚\n"
        "ä½ çš„ä»»åŠ¡æ˜¯ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·æä¾›çš„ç»“æ„æ¨¡æ¿è¾“å‡ºæ•´ç†ç»“æœã€‚"
        "å¯¹é¢‘ç¹å‡ºç°çš„æ•°å€¼ï¼Œä»…æå–ï¼š\n"
        "- å¸¸è§å€¼ / å¹³å‡å€¼\n"
        "- æ³¢åŠ¨èŒƒå›´ï¼ˆæœ€ä½ ~ æœ€é«˜ï¼‰\n"
        "- æ³¢åŠ¨å¹…åº¦ï¼ˆæœ€é«˜ - æœ€ä½ï¼‰\n"
        "ç¼ºå¤±ä¿¡æ¯å¡«â€œæœªæåŠâ€ã€‚\n"
        "ç¦æ­¢è¾“å‡ºï¼šå¤šä½™è¯´æ˜ã€åˆ†æã€è¡¨æ ¼ã€æ³¨é‡Šã€æ ¼å¼æç¤ºã€æ¨¡æ¿é‡å¤ã€‚\n"
    )
    system_ids = tokenizer(system_content, return_tensors=None, add_special_tokens=False)["input_ids"]

    # æ„é€ ç»“æ„åŒ–æ¨¡æ¿å‰ç¼€ï¼ˆåŒ…å« raw_text çš„å‰ç¼€ï¼‰
    prefix = "è¯·æ ¹æ®ä»¥ä¸‹ç—…æƒ…æè¿°å†…å®¹æ•´ç†å‡ºç»“æ„åŒ–ç»“æœï¼š\n\n"
    prefix_ids = tokenizer(prefix, return_tensors=None, add_special_tokens=False)["input_ids"]

    # æ„é€ ç»“æ„æ¨¡æ¿åç¼€
    structure_suffix = (
        "\nè¯·å°†æå–ç»“æœä»…å¡«å…¥ä»¥ä¸‹ã€ç»“æ„æ¨¡æ¿ã€‘ä¸­ã€‚\n"

        "ã€ç»“æ„æ¨¡æ¿ã€‘ï¼š\n\n"
        "1. **è¡€ç³–æ§åˆ¶**\n"
        "- ç©ºè…¹è¡€ç³–æ³¢åŠ¨æƒ…å†µï¼š\n"
        " - å¹³å‡å€¼ï¼š\n"
        " - æ³¢åŠ¨èŒƒå›´ï¼š\n"
        " - æ³¢åŠ¨å¹…åº¦ï¼š\n"
        "- é¤åè¡€ç³–æ³¢åŠ¨æƒ…å†µï¼š\n"
        " - å¹³å‡å€¼ï¼š\n"
        " - æ³¢åŠ¨èŒƒå›´ï¼š\n"
        " - æ³¢åŠ¨å¹…åº¦ï¼š\n"
        "- ç—‡çŠ¶ï¼š\n"
        "2. **è¡€å‹ç®¡ç†**\n"
        "- æ”¶ç¼©å‹ï¼š\n"
        "- èˆ’å¼ å‹ï¼š\n"
        "- ç©ºè…¹è¡€å‹ï¼š\n"
        "- é¤åè¡€å‹ï¼š\n"
        "3. **ä¾ä»æ€§ä¸ç›‘æµ‹é—®é¢˜**\n"
        "- ä¾ä»æ€§ï¼š\n"
        "- ç”¨è¯æƒ…å†µï¼š\n"
        "4. **ç”Ÿæ´»æ–¹å¼**\n"
        "- é¥®é£Ÿï¼š\n"
        "- è¿åŠ¨ï¼š\n"
        "- ä½“é‡å˜åŒ–ï¼š\n\n"

        "\nã€ä»…å¡«å†™ä»¥ä¸Šæ¨¡æ¿å­—æ®µï¼Œè¾“å‡ºåˆ°æ­¤ä¸ºæ­¢ã€‚ã€‘"
    )
    suffix_ids = tokenizer(structure_suffix, return_tensors=None, add_special_tokens=False)["input_ids"]

    # ç»™ raw_text å‰©ä¸‹çš„ token budget
    reserved = len(system_ids) + len(prefix_ids) + len(suffix_ids)
    available_budget = max_input_tokens - reserved
    if available_budget <= 0:
        raise ValueError(f"[é”™è¯¯] max_input_tokens={max_input_tokens} å¤ªå°ï¼Œä¸è¶³ä»¥å®¹çº³æ¨¡æ¿å›ºå®šéƒ¨åˆ†")

    # æˆªæ–­ raw_text
    raw_text_ids = tokenizer(raw_text, return_tensors=None, add_special_tokens=False)["input_ids"]
    if len(raw_text_ids) > available_budget:
        raw_text_ids = raw_text_ids[:available_budget]

    # æ‹¼æ¥æˆå®Œæ•´ input_ids
    full_ids = system_ids + prefix_ids + raw_text_ids + suffix_ids
    return full_ids


# æ£€æŸ¥è¾“å‡ºç»“æ„å®Œæ•´æ€§
def is_valid_output(output, min_sections=4):
    required_sections = [
        "**è¡€ç³–æ§åˆ¶**",
        "**è¡€å‹ç®¡ç†**",
        "**ä¾ä»æ€§ä¸ç›‘æµ‹é—®é¢˜**",
        "**ç”Ÿæ´»æ–¹å¼**"
    ]
    matched = [s for s in required_sections if s in output]
    return len(matched), len(matched) >= min_sections


# åå¤„ç†éƒ¨åˆ†
def process_response(text):
    # ç§»é™¤ <think> éƒ¨åˆ†
    marker = "</think>"
    pos = text.find(marker)
    if pos != -1:
        end_index = pos + len(marker)
        while end_index < len(text) and text[end_index] == "\n":
            end_index += 1
        text = text[end_index:]

    # åˆ é™¤æ‰€æœ‰å‡ºç°çš„â€œã€ç»“æ„æ¨¡ç‰ˆã€‘â€æˆ–â€œã€ç»“æ„æ¨¡æ¿ã€‘â€
    text = text.replace("ã€ç»“æ„æ¨¡ç‰ˆã€‘", "").replace("ã€ç»“æ„æ¨¡æ¿ã€‘", "")

    # å»é™¤æ¨¡å‹å°¾éƒ¨æ„å¤–é‡å¤æ¨¡æ¿
    template_signals = [
        "è¯·æ ¹æ®æä¾›çš„ç—…æƒ…æè¿°å†…å®¹æ•´ç†å‡ºç»“æ„åŒ–ç»“æœ",
        "ç»“æ„æ¨¡ç‰ˆï¼š",
        "è¯·å°†æå–ç»“æœ"
    ]
    for key in template_signals:
        if key in text:
            text = text.split(key)[0].strip()

    # æ¸…æ´—é‡å¤ checklist æ®µ
    lines = text.splitlines()
    deduped = []
    seen = set()
    for line in lines:
        if line.strip() not in seen:
            deduped.append(line)
            seen.add(line.strip())

    return "\n".join(deduped)

    
# å•æ¡å¤„ç†éƒ¨åˆ†
def query_llm_single(raw_text, max_new_tokens=2200):
    MAX_INPUT_TOKENS = 8000
    MODEL_MAX_LENGTH = 16384

    full_ids = build_prompt_tokens(raw_text, tokenizer, MAX_INPUT_TOKENS)
    total_len = len(full_ids)

    if total_len + max_new_tokens > MODEL_MAX_LENGTH:
        allowed = MODEL_MAX_LENGTH - max_new_tokens
        print(f"[ç¡¬æ€§æˆªæ–­] prompt æ€» token æ•°ä¸º {total_len}ï¼Œè¶…è¿‡æœ€å¤§å€¼ {MODEL_MAX_LENGTH - max_new_tokens}ï¼Œæˆªä¸ºå‰ {allowed} ä¸ª token")
        full_ids = full_ids[:allowed]

    input_ids = torch.tensor(full_ids)
    attention_mask = torch.ones_like(input_ids)

    # å·¦ paddingï¼ˆè¿™é‡Œåªå¤„ç†å•æ¡ï¼Œå› æ­¤ä¸ç”¨ stackï¼‰
    max_len = input_ids.size(0)
    input_ids_padded = pad(input_ids, (0, 0), value=tokenizer.pad_token_id).unsqueeze(0)
    attention_mask_padded = pad(attention_mask, (0, 0), value=0).unsqueeze(0)

    encodings = {
        "input_ids": input_ids_padded.to(device),
        "attention_mask": attention_mask_padded.to(device)
    }

    with torch.no_grad():
        outputs = model.generate(
            **encodings,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
        )

    input_len = (encodings["attention_mask"] == 1).sum(dim=1)[0]
    gen_ids = outputs[0][input_len:]
    text = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return process_response(text)


# è¾“å…¥éƒ¨åˆ†
print("\nè¯·è¾“å…¥æ‚£è€…ç—…æƒ…æè¿°ï¼ˆè¾“å…¥ q å›è½¦é€€å‡ºï¼‰ï¼š")
while True:
    raw = input("\nğŸ“ æ‚£è€…æƒ…å†µï¼š\n")
    if raw.lower() in ['q', 'quit', 'exit']:
        break
    try:
        result = query_llm_single(raw)
        matched_count, is_valid = is_valid_output(result)
        print("\nğŸ§¾ æ¨¡å‹è¾“å‡ºï¼š\n")
        print(result)
        print(f"\nâœ… åŒ¹é…å­—æ®µæ•°ï¼š{matched_count}ï¼ŒçŠ¶æ€ï¼š{'FULL' if is_valid else 'PARTIAL' if matched_count >= 2 else 'FAIL'}")
    except Exception as e:
        print(f"[âŒ é”™è¯¯] å¤„ç†å¤±è´¥ï¼š{e}")

    # æ˜¾å­˜æ¸…ç†
    torch.cuda.empty_cache()
    gc.collect()


# åŠ è½½æ•°æ®
df = pd.read_excel("./input/joined_condition.xlsx")
try:
    df_out = pd.read_excel("./output/DS_lv2_cut/joined_DSlv2_text_cut.xlsx")
    processed_indices = set(df_out.index[df_out['response'].notna()])
    print(f"æ£€æµ‹åˆ°å·²æœ‰ {len(processed_indices)} æ¡å·²å¤„ç†è®°å½•ï¼Œå°†è·³è¿‡è¿™äº›è®°å½•ã€‚")
except FileNotFoundError:
    df_out = df.copy()
    df_out['response'] = ""
    df_out['status'] = ""
    processed_indices = set()

batch_size = 15 #æ§åˆ¶æ‰¹æ¬¡å¤§å°
error_records = []
total = len(df)
start_time = time.time()

# æ ‡è®°è¾“å‡ºæƒ…å†µï¼Œä¿ç•™é”™è¯¯æˆ–æˆªæ–­è¾“å‡ºï¼Œæ–¹ä¾¿ç»Ÿè®¡
for start_idx in range(0, total, batch_size):
    end_idx = min(start_idx + batch_size, total)
    batch_indices = [i for i in range(start_idx, end_idx) if i not in processed_indices]
    if not batch_indices:
        continue

    batch_prompts = [str(df.loc[i, 'patient_condition']) for i in batch_indices]

    try:
        batch_outputs = query_llm_batch(batch_prompts)
        for i, output in zip(batch_indices, batch_outputs):
            matched_count, is_valid = is_valid_output(output)
            df_out.at[i, 'response'] = output
            if is_valid:
                df_out.at[i, 'status'] = "FULL"
            elif matched_count >= 2:
                df_out.at[i, 'status'] = f"PARTIAL_{matched_count}"
            else:
                df_out.at[i, 'status'] = "FAIL"
                error_records.append((i, batch_prompts[i - start_idx], output))
    except Exception as e:
        for i in batch_indices:
            df_out.at[i, 'response'] = f"[ERROR] {e}"
            df_out.at[i, 'status'] = "ERROR"
            error_records.append((i, df.loc[i, 'patient_condition'], str(e)))

    completed = end_idx
    elapsed = time.time() - start_time
    # é‡Šæ”¾æ˜¾å­˜ï¼ˆè¿™æ®µçœŸçš„å¾ˆé‡è¦ï¼‰
    torch.cuda.empty_cache()
    gc.collect()
    time.sleep(1) #ç¡®ä¿æ˜¾å­˜é‡Šæ”¾å¹²å‡€ï¼ˆç§’ï¼‰ï¼Œå¦‚æœè¿˜æœ‰çˆ†æ˜¾å­˜çš„æƒ…å†µå°±åŠ åˆ°1.25
    print(f"DeepSeek_lv2å·²å®Œæˆ {completed}/{total} æ¡ï¼Œè€—æ—¶ {elapsed:.2f} ç§’")

    # åœ¨æ˜¾å­˜å¤§äº 40GB æ—¶é‡Šæ”¾æ˜¾å­˜
#    if torch.cuda.is_available():
#        current_mem = torch.cuda.memory_allocated() / (1024 ** 3)
#        if current_mem > 40:
#            print(f"[æ˜¾å­˜é‡Šæ”¾] å½“å‰å ç”¨çº¦ {current_mem:.2f} GBï¼Œæ‰§è¡Œé‡Šæ”¾...")
#            torch.cuda.empty_cache()
#            gc.collect()

    if completed % 15 == 0:
        df_out.to_excel("./output/DS_lv2_cut/joined_DSlv2_text_cut.xlsx", index=False)


df_out.to_excel("./output/DS_lv2_cut/joined_DSlv2_text_cut.xlsx", index=False)

if error_records:
    df_error = pd.DataFrame(error_records, columns=["index", "prompt", "error"])
    df_error.to_excel("./output/DS_lv2_cut/deepseek_errors_cut.xlsx", index=False)
    print(f"æœ‰ {len(error_records)} æ¡æ•°æ®å¤„ç†å¤±è´¥ï¼Œè¯¦è§ deepseek_errors_batch8.xlsx")

print("#### æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆ ####")